#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SERVICE OLLAMA - INT√âGRATION LLM LOCAL
=====================================

Service pour l'int√©gration avec Ollama (Mistral) pour l'analyse
intelligente des donn√©es de consommation √©lectrique Malaysia

Version: 1.0.0
"""

import json
import logging
import requests
import time
from typing import Dict, List, Optional, Any, Generator
from datetime import datetime

logger = logging.getLogger(__name__)


class OllamaService:
    """Service d'int√©gration avec Ollama pour l'analyse LLM"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        """
        Initialise le service Ollama
        
        Args:
            base_url: URL de base d'Ollama
        """
        self.base_url = base_url
        self.model = "mistral:latest"
        self.session = requests.Session()
        self.session.timeout = 120
        
        self._check_ollama_availability()
        logger.info("‚úÖ OllamaService initialis√©")
    
    def _check_ollama_availability(self):
        """V√©rifie la disponibilit√© d'Ollama"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                
                if self.model in model_names:
                    logger.info(f"‚úÖ Mod√®le {self.model} disponible")
                else:
                    logger.warning(f"‚ö†Ô∏è Mod√®le {self.model} non trouv√©")
                    logger.info(f"Mod√®les disponibles: {model_names}")
            else:
                logger.error("‚ùå Ollama non accessible")
        except Exception as e:
            logger.error(f"‚ùå Erreur connexion Ollama: {e}")
    
    def analyze_data(
        self, 
        question: str, 
        context: List[Dict], 
        data_summary: Dict
    ) -> Dict:
        """
        Analyse des donn√©es via LLM avec contexte RAG
        
        Args:
            question: Question utilisateur
            context: Contexte RAG
            data_summary: R√©sum√© des donn√©es
            
        Returns:
            Dict: R√©sultat de l'analyse
        """
        try:
            # Construction du prompt intelligent
            prompt = self._build_analysis_prompt(question, context, data_summary)
            
            # Appel au mod√®le
            response = self._call_ollama(prompt)
            
            # Parsing de la r√©ponse
            analysis = self._parse_analysis_response(response)
            
            return {
                'success': True,
                'analysis': analysis,
                'model_used': self.model,
                'timestamp': datetime.now().isoformat(),
                'prompt_tokens': len(prompt.split()),
                'context_items': len(context)
            }
            
        except Exception as e:
            logger.error(f"Erreur analyse: {e}")
            return {
                'success': False,
                'error': str(e),
                'fallback_analysis': self._generate_fallback_analysis(question, data_summary)
            }
    
    def analyze_data_stream(
        self, 
        question: str, 
        context: List[Dict], 
        data_summary: Dict
    ) -> Generator[Dict, None, None]:
        """
        Analyse en streaming pour r√©ponses temps r√©el
        
        Args:
            question: Question utilisateur
            context: Contexte RAG
            data_summary: R√©sum√© des donn√©es
            
        Yields:
            Dict: Chunks de r√©ponse en streaming
        """
        try:
            prompt = self._build_analysis_prompt(question, context, data_summary)
            
            for chunk in self._call_ollama_stream(prompt):
                yield {
                    'type': 'chunk',
                    'content': chunk,
                    'timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            yield {
                'type': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _build_analysis_prompt(
        self, 
        question: str, 
        context: List[Dict], 
        data_summary: Dict
    ) -> str:
        """Construit un prompt intelligent pour l'analyse"""
        
        # Template de prompt sp√©cialis√© pour l'analyse √©nerg√©tique
        prompt_template = """Tu es un expert en analyse de donn√©es √©nerg√©tiques pour la Malaisie.

CONTEXTE DES DONN√âES:
- Dataset: Consommation √©lectrique et m√©tadonn√©es de b√¢timents Malaysia
- P√©riode: {period}
- B√¢timents: {buildings_count}
- Types de b√¢timents: {building_types}
- Zones g√©ographiques: {zones}

R√âSUM√â STATISTIQUE:
{data_summary}

CONTEXTE RAG PERTINENT:
{rag_context}

QUESTION DE L'UTILISATEUR:
{question}

INSTRUCTIONS:
1. Analyse les donn√©es de mani√®re pr√©cise et factuelle
2. Utilise le contexte RAG pour enrichir ta r√©ponse
3. Fournis des insights actionnables
4. Inclus des m√©triques sp√©cifiques quand possible
5. Sugg√®re des visualisations pertinentes
6. Format ta r√©ponse en sections claires

R√âPONSE STRUCTUR√âE:"""

        # Extraction des informations cl√©s
        period = data_summary.get('period', 'Non sp√©cifi√©')
        buildings_count = data_summary.get('total_buildings', 0)
        building_types = ', '.join(data_summary.get('building_types', []))
        zones = ', '.join(data_summary.get('zones', []))
        
        # Formatage du contexte RAG
        rag_context = "\n".join([
            f"- {item.get('content', '')}" for item in context[:5]
        ]) if context else "Aucun contexte sp√©cifique trouv√©"
        
        # Formatage du r√©sum√©
        summary_text = json.dumps(data_summary, indent=2, ensure_ascii=False)
        
        return prompt_template.format(
            period=period,
            buildings_count=buildings_count,
            building_types=building_types,
            zones=zones,
            data_summary=summary_text,
            rag_context=rag_context,
            question=question
        )
    
    def _call_ollama(self, prompt: str) -> str:
        """Appel synchrone √† Ollama"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,  # R√©ponses plus factuelles
                    "top_p": 0.9,
                    "top_k": 40,
                    "num_predict": 2048
                }
            }
            
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                raise Exception(f"Erreur Ollama: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Erreur appel Ollama: {e}")
            raise
    
    def _call_ollama_stream(self, prompt: str) -> Generator[str, None, None]:
        """Appel streaming √† Ollama"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "top_k": 40
                }
            }
            
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                stream=True
            )
            
            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line.decode('utf-8'))
                        if 'response' in chunk:
                            yield chunk['response']
                        if chunk.get('done', False):
                            break
                    except json.JSONDecodeError:
                        continue
                        
        except Exception as e:
            logger.error(f"Erreur streaming Ollama: {e}")
            raise
    
    def _parse_analysis_response(self, response: str) -> Dict:
        """Parse et structure la r√©ponse d'analyse"""
        try:
            # Extraction des sections principales
            sections = {}
            current_section = "overview"
            current_content = []
            
            lines = response.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # D√©tection des sections
                if any(keyword in line.lower() for keyword in [
                    'r√©sum√©', 'synth√®se', 'insights', 'recommandations', 
                    'm√©triques', 'tendances', 'conclusions'
                ]):
                    if current_content:
                        sections[current_section] = '\n'.join(current_content)
                    current_section = line.lower().replace(':', '').strip()
                    current_content = []
                else:
                    if line:
                        current_content.append(line)
            
            # Ajout de la derni√®re section
            if current_content:
                sections[current_section] = '\n'.join(current_content)
            
            # Structure finale
            parsed = {
                'full_response': response,
                'sections': sections,
                'summary': sections.get('overview', response[:200] + '...'),
                'insights': self._extract_insights(response),
                'recommendations': self._extract_recommendations(response),
                'metrics': self._extract_metrics(response)
            }
            
            return parsed
            
        except Exception as e:
            logger.error(f"Erreur parsing r√©ponse: {e}")
            return {
                'full_response': response,
                'sections': {'raw': response},
                'summary': response[:200] + '...',
                'insights': [],
                'recommendations': [],
                'metrics': {}
            }
    
    def _extract_insights(self, response: str) -> List[str]:
        """Extrait les insights cl√©s de la r√©ponse"""
        insights = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            # Recherche de patterns d'insights
            if any(pattern in line.lower() for pattern in [
                'on observe', 'il appara√Æt', 'les donn√©es montrent',
                'tendance', 'pic', 'baisse', 'augmentation'
            ]):
                insights.append(line)
        
        return insights[:5]  # Limiter √† 5 insights
    
    def _extract_recommendations(self, response: str) -> List[str]:
        """Extrait les recommandations de la r√©ponse"""
        recommendations = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            # Recherche de patterns de recommandations
            if any(pattern in line.lower() for pattern in [
                'recommande', 'sugg√®re', 'devrait', 'pourrait',
                'optimiser', 'am√©liorer', 'r√©duire'
            ]):
                recommendations.append(line)
        
        return recommendations[:3]  # Limiter √† 3 recommandations
    
    def _extract_metrics(self, response: str) -> Dict:
        """Extrait les m√©triques num√©riques de la r√©ponse"""
        import re
        
        metrics = {}
        
        # Patterns pour extraire des m√©triques
        number_patterns = [
            r'(\d+(?:\.\d+)?)\s*%',  # Pourcentages
            r'(\d+(?:\.\d+)?)\s*kWh',  # Kilowatt-heures
            r'(\d+(?:\.\d+)?)\s*MW',   # Megawatts
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, response)
            if matches:
                if 'kWh' in pattern:
                    metrics['energy_kwh'] = [float(m) for m in matches]
                elif 'MW' in pattern:
                    metrics['power_mw'] = [float(m) for m in matches]
                elif '%' in pattern:
                    metrics['percentages'] = [float(m) for m in matches]
        
        return metrics
    
    def _generate_fallback_analysis(self, question: str, data_summary: Dict) -> str:
        """G√©n√®re une analyse de secours en cas d'erreur LLM"""
        
        fallback = f"""
ANALYSE DE SECOURS - Donn√©es Malaysia

Question: {question}

R√©sum√© des donn√©es disponibles:
- B√¢timents total: {data_summary.get('total_buildings', 'N/A')}
- P√©riode: {data_summary.get('period', 'N/A')}
- Types de b√¢timents: {', '.join(data_summary.get('building_types', []))}

Note: Cette analyse simplifi√©e est g√©n√©r√©e en cas d'indisponibilit√© du mod√®le LLM.
Pour une analyse compl√®te, v√©rifiez la connexion √† Ollama.
"""
        
        return fallback.strip()
    
    def get_model_info(self) -> Dict:
        """Informations sur le mod√®le utilis√©"""
        try:
            response = self.session.get(f"{self.base_url}/api/show", 
                                      json={"name": self.model})
            if response.status_code == 200:
                return response.json()
            else:
                return {"error": "Mod√®le non disponible"}
        except Exception as e:
            return {"error": str(e)}
    
    def health_check(self) -> Dict:
        """V√©rification de sant√© du service"""
        try:
            start_time = time.time()
            response = self.session.get(f"{self.base_url}/api/tags")
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                return {
                    'status': 'healthy',
                    'response_time_ms': round(response_time * 1000, 2),
                    'models_available': len(models),
                    'target_model_available': any(m['name'] == self.model for m in models)
                }
            else:
                return {
                    'status': 'unhealthy',
                    'error': f"HTTP {response.status_code}"
                }
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e)
            }


# ==============================================================================
# UTILITAIRES
# ==============================================================================

def test_ollama_connection():
    """Test de connexion √† Ollama"""
    service = OllamaService()
    health = service.health_check()
    
    print("üîç Test connexion Ollama:")
    print(f"Status: {health.get('status')}")
    if health.get('response_time_ms'):
        print(f"Temps de r√©ponse: {health['response_time_ms']}ms")
    if health.get('error'):
        print(f"Erreur: {health['error']}")
    
    return health.get('status') == 'healthy'


if __name__ == '__main__':
    test_ollama_connection()