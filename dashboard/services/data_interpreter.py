#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
INTERPR√âTEUR DE DONN√âES - DASHBOARD MALAYSIA
===========================================

Service d'interpr√©tation automatique des donn√©es par le LLM
pour g√©n√©rer des insights intelligents d√®s le chargement

Version: 1.0.0
"""

import logging
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import sqlite3
from pathlib import Path

logger = logging.getLogger(__name__)


class DataInterpreter:
    """Service d'interpr√©tation automatique des donn√©es par LLM"""
    
    def __init__(self, ollama_service, rag_service, data_service):
        """
        Initialise l'interpr√©teur de donn√©es
        
        Args:
            ollama_service: Service Ollama pour l'IA
            rag_service: Service RAG pour le contexte
            data_service: Service de donn√©es
        """
        self.ollama_service = ollama_service
        self.rag_service = rag_service
        self.data_service = data_service
        
        # Base de donn√©es pour stocker les interpr√©tations
        self.db_path = Path("data/interpretations.db")
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_interpretations_db()
        
        # Templates d'analyse par type de donn√©es
        self.analysis_templates = {
            'overview': self._get_overview_template(),
            'consumption': self._get_consumption_template(),
            'buildings': self._get_buildings_template(),
            'weather': self._get_weather_template(),
            'water': self._get_water_template(),
            'trends': self._get_trends_template(),
            'anomalies': self._get_anomalies_template(),
            'recommendations': self._get_recommendations_template()
        }
        
        logger.info("‚úÖ DataInterpreter initialis√©")
    
    def _init_interpretations_db(self):
        """Initialise la base de donn√©es des interpr√©tations"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS interpretations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    interpretation_type TEXT,
                    data_hash TEXT,
                    analysis_content TEXT,
                    insights TEXT,
                    recommendations TEXT,
                    confidence_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    metadata TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS interpretation_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT,
                    interpretation_id INTEGER,
                    user_feedback TEXT,
                    usefulness_score INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (interpretation_id) REFERENCES interpretations (id)
                )
            """)
            
            conn.commit()
    
    async def interpret_loaded_data(self, force_refresh: bool = False) -> Dict:
        """
        Lance l'interpr√©tation compl√®te des donn√©es charg√©es
        
        Args:
            force_refresh: Force une nouvelle analyse m√™me si d√©j√† fait
            
        Returns:
            Dict: R√©sultats de toutes les interpr√©tations
        """
        try:
            logger.info("üß† D√©marrage interpr√©tation automatique des donn√©es...")
            
            # R√©cup√©ration des donn√©es actuelles
            current_data = self.data_service.get_current_data()
            data_summary = self.data_service.get_data_summary()
            
            if not any(current_data.values()):
                return {
                    'success': False,
                    'error': 'Aucune donn√©e charg√©e √† interpr√©ter'
                }
            
            # Calcul du hash des donn√©es pour √©viter les analyses r√©p√©t√©es
            data_hash = self._calculate_data_hash(current_data)
            
            if not force_refresh and self._interpretation_exists(data_hash):
                logger.info("üìã Interpr√©tation existante trouv√©e, r√©cup√©ration...")
                return self._get_existing_interpretation(data_hash)
            
            # Analyses par composant
            interpretations = {}
            
            # 1. Vue d'ensemble g√©n√©rale
            interpretations['overview'] = await self._analyze_overview(
                current_data, data_summary
            )
            
            # 2. Analyse de consommation
            if current_data.get('consumption') is not None:
                interpretations['consumption'] = await self._analyze_consumption(
                    current_data['consumption'], current_data.get('buildings')
                )
            
            # 3. Analyse des b√¢timents
            if current_data.get('buildings') is not None:
                interpretations['buildings'] = await self._analyze_buildings(
                    current_data['buildings']
                )
            
            # 4. Corr√©lations m√©t√©o (si disponible)
            if current_data.get('weather') is not None:
                interpretations['weather'] = await self._analyze_weather_correlation(
                    current_data['weather'], current_data.get('consumption')
                )
            
            # 5. D√©tection d'anomalies
            interpretations['anomalies'] = await self._detect_anomalies(
                current_data
            )
            
            # 6. Tendances temporelles
            interpretations['trends'] = await self._analyze_trends(
                current_data
            )
            
            # 7. Recommandations strat√©giques
            interpretations['recommendations'] = await self._generate_recommendations(
                current_data, data_summary, interpretations
            )
            
            # Sauvegarde des r√©sultats
            self._save_interpretation_results(data_hash, interpretations)
            
            # Cr√©ation d'un r√©sum√© ex√©cutif
            executive_summary = await self._create_executive_summary(interpretations)
            
            final_result = {
                'success': True,
                'interpretations': interpretations,
                'executive_summary': executive_summary,
                'analysis_timestamp': datetime.now().isoformat(),
                'data_hash': data_hash,
                'confidence_score': self._calculate_overall_confidence(interpretations)
            }
            
            logger.info("‚úÖ Interpr√©tation automatique termin√©e")
            return final_result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur interpr√©tation donn√©es: {e}")
            return {
                'success': False,
                'error': str(e),
                'fallback_analysis': self._generate_fallback_analysis(data_summary)
            }
    
    async def _analyze_overview(self, data: Dict, summary: Dict) -> Dict:
        """Analyse vue d'ensemble des donn√©es"""
        try:
            # Pr√©paration du contexte statistique
            stats_context = f"""
R√âSUM√â DES DONN√âES MALAYSIA:
- P√©riode analys√©e: {summary.get('period', 'Non d√©finie')}
- Nombre total de b√¢timents: {summary.get('total_buildings', 0):,}
- Types de b√¢timents: {', '.join(summary.get('building_types', []))}
- Zones g√©ographiques: {', '.join(summary.get('zones', []))}
- Consommation totale: {summary.get('total_consumption', 0):,.0f} kWh
- Consommation moyenne: {summary.get('avg_consumption', 0):.1f} kWh
- Points de donn√©es: {summary.get('total_data_points', 0):,}

DISPONIBILIT√â DES DONN√âES:
- B√¢timents: {'‚úì' if summary.get('data_availability', {}).get('buildings') else '‚úó'}
- Consommation: {'‚úì' if summary.get('data_availability', {}).get('consumption') else '‚úó'}
- M√©t√©o: {'‚úì' if summary.get('data_availability', {}).get('weather') else '‚úó'}
- Eau: {'‚úì' if summary.get('data_availability', {}).get('water') else '‚úó'}
"""
            
            # Recherche de contexte RAG pertinent
            rag_context = self.rag_service.search_context(
                "vue d'ensemble donn√©es √©nerg√©tiques Malaysia", top_k=3
            )
            
            # Construction du prompt d'analyse
            prompt = self.analysis_templates['overview'].format(
                stats_context=stats_context,
                rag_context=self._format_rag_context(rag_context)
            )
            
            # Analyse par le LLM
            analysis = self.ollama_service.analyze_data(
                question="Analyse et interpr√®te cette vue d'ensemble des donn√©es √©nerg√©tiques",
                context=rag_context,
                data_summary=summary
            )
            
            if analysis.get('success'):
                return {
                    'type': 'overview',
                    'content': analysis['analysis']['full_response'],
                    'insights': analysis['analysis'].get('insights', []),
                    'confidence': 0.9,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                raise Exception(f"√âchec analyse LLM: {analysis.get('error')}")
                
        except Exception as e:
            logger.error(f"Erreur analyse overview: {e}")
            return {
                'type': 'overview',
                'content': f"Analyse automatique non disponible: {e}",
                'insights': [],
                'confidence': 0.1,
                'timestamp': datetime.now().isoformat()
            }
    
    async def _analyze_consumption(self, consumption_df: pd.DataFrame, 
                                 buildings_df: pd.DataFrame = None) -> Dict:
        """Analyse d√©taill√©e de la consommation"""
        try:
            # Calcul de statistiques avanc√©es
            consumption_stats = self._calculate_consumption_statistics(consumption_df)
            
            # Analyse temporelle
            temporal_analysis = self._analyze_temporal_patterns(consumption_df)
            
            # Corr√©lation avec les b√¢timents si disponible
            building_correlation = ""
            if buildings_df is not None:
                building_correlation = self._analyze_building_consumption_correlation(
                    consumption_df, buildings_df
                )
            
            # Construction du contexte
            analysis_context = f"""
STATISTIQUES DE CONSOMMATION D√âTAILL√âES:
{consumption_stats}

ANALYSE TEMPORELLE:
{temporal_analysis}

{building_correlation}
"""
            
            # Recherche RAG contexte
            rag_context = self.rag_service.search_context(
                "analyse consommation √©lectrique patterns temporels", top_k=3
            )
            
            # Prompt sp√©cialis√© consommation
            prompt = self.analysis_templates['consumption'].format(
                analysis_context=analysis_context,
                rag_context=self._format_rag_context(rag_context)
            )
            
            # Analyse LLM
            analysis = self.ollama_service.analyze_data(
                question="Analyse en d√©tail ces patterns de consommation √©lectrique",
                context=rag_context,
                data_summary={'consumption_data': analysis_context}
            )
            
            insights = []
            if analysis.get('success'):
                content = analysis['analysis']['full_response']
                insights = analysis['analysis'].get('insights', [])
                
                # Ajout d'insights calcul√©s
                insights.extend(self._extract_consumption_insights(consumption_df))
                
                return {
                    'type': 'consumption',
                    'content': content,
                    'insights': insights,
                    'statistics': consumption_stats,
                    'confidence': 0.85,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                raise Exception("√âchec analyse consommation")
                
        except Exception as e:
            logger.error(f"Erreur analyse consommation: {e}")
            return {
                'type': 'consumption',
                'content': f"Analyse consommation indisponible: {e}",
                'insights': [],
                'confidence': 0.1,
                'timestamp': datetime.now().isoformat()
            }
    
    async def _analyze_buildings(self, buildings_df: pd.DataFrame) -> Dict:
        """Analyse du patrimoine de b√¢timents"""
        try:
            # Statistiques par type
            type_analysis = self._analyze_building_types(buildings_df)
            
            # Analyse g√©ographique
            geo_analysis = self._analyze_geographic_distribution(buildings_df)
            
            # Analyse des surfaces
            surface_analysis = self._analyze_surface_distribution(buildings_df)
            
            buildings_context = f"""
ANALYSE DU PATRIMOINE B√ÇTIMENTS:

R√âPARTITION PAR TYPE:
{type_analysis}

DISTRIBUTION G√âOGRAPHIQUE:
{geo_analysis}

ANALYSE DES SURFACES:
{surface_analysis}
"""
            
            rag_context = self.rag_service.search_context(
                "analyse b√¢timents types patrimoine immobilier", top_k=3
            )
            
            prompt = self.analysis_templates['buildings'].format(
                buildings_context=buildings_context,
                rag_context=self._format_rag_context(rag_context)
            )
            
            analysis = self.ollama_service.analyze_data(
                question="Analyse le patrimoine de b√¢timents et ses caract√©ristiques",
                context=rag_context,
                data_summary={'buildings_data': buildings_context}
            )
            
            if analysis.get('success'):
                return {
                    'type': 'buildings',
                    'content': analysis['analysis']['full_response'],
                    'insights': analysis['analysis'].get('insights', []),
                    'building_statistics': {
                        'types': type_analysis,
                        'geography': geo_analysis,
                        'surfaces': surface_analysis
                    },
                    'confidence': 0.8,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                raise Exception("√âchec analyse b√¢timents")
                
        except Exception as e:
            logger.error(f"Erreur analyse b√¢timents: {e}")
            return {
                'type': 'buildings',
                'content': f"Analyse b√¢timents indisponible: {e}",
                'insights': [],
                'confidence': 0.1,
                'timestamp': datetime.now().isoformat()
            }
    
    async def _detect_anomalies(self, data: Dict) -> Dict:
        """D√©tection d'anomalies dans les donn√©es"""
        try:
            anomalies_found = []
            
            # Anomalies de consommation
            if data.get('consumption') is not None:
                consumption_anomalies = self._detect_consumption_anomalies(
                    data['consumption']
                )
                anomalies_found.extend(consumption_anomalies)
            
            # Anomalies de b√¢timents
            if data.get('buildings') is not None:
                building_anomalies = self._detect_building_anomalies(
                    data['buildings']
                )
                anomalies_found.extend(building_anomalies)
            
            # Incoh√©rences entre datasets
            cross_anomalies = self._detect_cross_dataset_anomalies(data)
            anomalies_found.extend(cross_anomalies)
            
            anomalies_context = f"""
ANOMALIES D√âTECT√âES:
Nombre total d'anomalies: {len(anomalies_found)}

D√âTAILS:
""" + "\n".join([f"- {anomaly}" for anomaly in anomalies_found])
            
            # Analyse par LLM des anomalies
            rag_context = self.rag_service.search_context(
                "anomalies donn√©es √©nerg√©tiques validation qualit√©", top_k=2
            )
            
            analysis = self.ollama_service.analyze_data(
                question="Analyse ces anomalies d√©tect√©es dans les donn√©es",
                context=rag_context,
                data_summary={'anomalies': anomalies_context}
            )
            
            if analysis.get('success'):
                return {
                    'type': 'anomalies',
                    'content': analysis['analysis']['full_response'],
                    'anomalies_list': anomalies_found,
                    'severity': 'low' if len(anomalies_found) < 5 else 'medium' if len(anomalies_found) < 15 else 'high',
                    'confidence': 0.7,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                raise Exception("√âchec analyse anomalies")
                
        except Exception as e:
            logger.error(f"Erreur d√©tection anomalies: {e}")
            return {
                'type': 'anomalies',
                'content': f"D√©tection anomalies indisponible: {e}",
                'anomalies_list': [],
                'confidence': 0.1,
                'timestamp': datetime.now().isoformat()
            }
    
    async def _generate_recommendations(self, data: Dict, summary: Dict, 
                                       interpretations: Dict) -> Dict:
        """G√©n√®re des recommandations strat√©giques"""
        try:
            # Compilation des insights de toutes les analyses
            all_insights = []
            for analysis in interpretations.values():
                if isinstance(analysis, dict) and 'insights' in analysis:
                    all_insights.extend(analysis.get('insights', []))
            
            # Contexte pour les recommandations
            recommendations_context = f"""
CONTEXTE POUR RECOMMANDATIONS:

DONN√âES DISPONIBLES:
- {summary.get('total_buildings', 0)} b√¢timents analys√©s
- {summary.get('total_consumption', 0):,.0f} kWh de consommation
- Types: {', '.join(summary.get('building_types', []))}

INSIGHTS D√âTECT√âS:
""" + "\n".join([f"‚Ä¢ {insight}" for insight in all_insights[:10]])  # Top 10 insights
            
            rag_context = self.rag_service.search_context(
                "recommandations optimisation √©nerg√©tique efficacit√©", top_k=4
            )
            
            analysis = self.ollama_service.analyze_data(
                question="G√©n√®re des recommandations strat√©giques d'optimisation √©nerg√©tique",
                context=rag_context,
                data_summary={'context': recommendations_context}
            )
            
            if analysis.get('success'):
                recommendations = self._parse_recommendations(
                    analysis['analysis']['full_response']
                )
                
                return {
                    'type': 'recommendations',
                    'content': analysis['analysis']['full_response'],
                    'recommendations': recommendations,
                    'priority_actions': recommendations[:3],  # Top 3
                    'confidence': 0.75,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                raise Exception("√âchec g√©n√©ration recommandations")
                
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration recommandations: {e}")
            return {
                'type': 'recommendations',
                'content': f"Recommandations indisponibles: {e}",
                'recommendations': [],
                'confidence': 0.1,
                'timestamp': datetime.now().isoformat()
            }
    
    async def _create_executive_summary(self, interpretations: Dict) -> Dict:
        """Cr√©e un r√©sum√© ex√©cutif de toutes les analyses"""
        try:
            # Compilation des points cl√©s
            key_points = []
            recommendations = []
            
            for analysis in interpretations.values():
                if isinstance(analysis, dict):
                    # Points cl√©s
                    insights = analysis.get('insights', [])
                    if insights:
                        key_points.extend(insights[:2])  # Top 2 par analyse
                    
                    # Recommandations
                    if analysis.get('type') == 'recommendations':
                        recs = analysis.get('recommendations', [])
                        recommendations.extend(recs[:3])  # Top 3
            
            summary_context = f"""
POINTS CL√âS IDENTIFI√âS:
""" + "\n".join([f"‚Ä¢ {point}" for point in key_points[:8]])
            
            summary_context += f"""

RECOMMANDATIONS PRIORITAIRES:
""" + "\n".join([f"‚Ä¢ {rec}" for rec in recommendations[:5]])
            
            # G√©n√©ration du r√©sum√© ex√©cutif
            rag_context = self.rag_service.search_context(
                "r√©sum√© ex√©cutif synth√®se analyse √©nerg√©tique", top_k=2
            )
            
            analysis = self.ollama_service.analyze_data(
                question="Cr√©e un r√©sum√© ex√©cutif concis de cette analyse √©nerg√©tique",
                context=rag_context,
                data_summary={'summary_context': summary_context}
            )
            
            if analysis.get('success'):
                return {
                    'executive_summary': analysis['analysis']['full_response'],
                    'key_metrics': self._extract_key_metrics(interpretations),
                    'top_insights': key_points[:5],
                    'priority_actions': recommendations[:3],
                    'confidence': 0.8,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                raise Exception("√âchec r√©sum√© ex√©cutif")
                
        except Exception as e:
            logger.error(f"Erreur r√©sum√© ex√©cutif: {e}")
            return {
                'executive_summary': f"R√©sum√© ex√©cutif indisponible: {e}",
                'key_metrics': {},
                'confidence': 0.1,
                'timestamp': datetime.now().isoformat()
            }
    
    # =========================================================================
    # M√âTHODES UTILITAIRES D'ANALYSE
    # =========================================================================
    
    def _calculate_consumption_statistics(self, consumption_df: pd.DataFrame) -> str:
        """Calcule des statistiques avanc√©es de consommation"""
        try:
            if 'y' not in consumption_df.columns:
                return "Colonne de consommation 'y' manquante"
            
            consumption = consumption_df['y']
            
            stats = f"""
‚Ä¢ Consommation totale: {consumption.sum():,.0f} kWh
‚Ä¢ Consommation moyenne: {consumption.mean():.1f} kWh
‚Ä¢ M√©diane: {consumption.median():.1f} kWh
‚Ä¢ √âcart-type: {consumption.std():.1f} kWh
‚Ä¢ Coefficient de variation: {(consumption.std()/consumption.mean()*100):.1f}%
‚Ä¢ Minimum: {consumption.min():.1f} kWh
‚Ä¢ Maximum: {consumption.max():.1f} kWh
‚Ä¢ 95e percentile: {consumption.quantile(0.95):.1f} kWh"""
            
            return stats
            
        except Exception as e:
            return f"Erreur calcul statistiques: {e}"
    
    def _analyze_temporal_patterns(self, consumption_df: pd.DataFrame) -> str:
        """Analyse les patterns temporels"""
        try:
            if 'timestamp' not in consumption_df.columns:
                return "Colonne timestamp manquante"
            
            consumption_df['timestamp'] = pd.to_datetime(consumption_df['timestamp'])
            consumption_df['hour'] = consumption_df['timestamp'].dt.hour
            consumption_df['day_of_week'] = consumption_df['timestamp'].dt.dayofweek
            
            # Patterns horaires
            hourly_avg = consumption_df.groupby('hour')['y'].mean()
            peak_hour = hourly_avg.idxmax()
            low_hour = hourly_avg.idxmin()
            
            # Patterns hebdomadaires
            daily_avg = consumption_df.groupby('day_of_week')['y'].mean()
            peak_day = daily_avg.idxmax()
            low_day = daily_avg.idxmin()
            
            day_names = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']
            
            patterns = f"""
‚Ä¢ Pic horaire: {peak_hour}h ({hourly_avg[peak_hour]:.1f} kWh moyenne)
‚Ä¢ Creux horaire: {low_hour}h ({hourly_avg[low_hour]:.1f} kWh moyenne)
‚Ä¢ Variation journali√®re: {((hourly_avg.max() - hourly_avg.min()) / hourly_avg.mean() * 100):.1f}%
‚Ä¢ Jour le plus consommateur: {day_names[peak_day]} ({daily_avg[peak_day]:.1f} kWh)
‚Ä¢ Jour le moins consommateur: {day_names[low_day]} ({daily_avg[low_day]:.1f} kWh)"""
            
            return patterns
            
        except Exception as e:
            return f"Erreur analyse temporelle: {e}"
    
    def _detect_consumption_anomalies(self, consumption_df: pd.DataFrame) -> List[str]:
        """D√©tecte les anomalies de consommation"""
        anomalies = []
        
        try:
            if 'y' not in consumption_df.columns:
                return ["Colonne consommation manquante"]
            
            consumption = consumption_df['y']
            
            # Valeurs n√©gatives
            negative_count = (consumption < 0).sum()
            if negative_count > 0:
                anomalies.append(f"{negative_count} valeurs de consommation n√©gatives")
            
            # Valeurs nulles
            zero_count = (consumption == 0).sum()
            if zero_count > len(consumption) * 0.05:  # > 5%
                anomalies.append(f"{zero_count} valeurs nulles ({zero_count/len(consumption)*100:.1f}%)")
            
            # Outliers extr√™mes (> 3 √©carts-types)
            z_scores = np.abs((consumption - consumption.mean()) / consumption.std())
            extreme_outliers = (z_scores > 3).sum()
            if extreme_outliers > 0:
                anomalies.append(f"{extreme_outliers} valeurs extr√™mes (>3œÉ)")
            
            # Variations brutales (> 10x la moyenne)
            if 'timestamp' in consumption_df.columns:
                consumption_sorted = consumption_df.sort_values('timestamp')
                diffs = consumption_sorted['y'].diff().abs()
                sudden_changes = (diffs > consumption.mean() * 10).sum()
                if sudden_changes > 0:
                    anomalies.append(f"{sudden_changes} variations brutales d√©tect√©es")
            
        except Exception as e:
            anomalies.append(f"Erreur d√©tection anomalies consommation: {e}")
        
        return anomalies
    
    def _detect_building_anomalies(self, buildings_df: pd.DataFrame) -> List[str]:
        """D√©tecte les anomalies dans les donn√©es de b√¢timents"""
        anomalies = []
        
        try:
            # Coordonn√©es invalides
            if 'latitude' in buildings_df.columns and 'longitude' in buildings_df.columns:
                invalid_coords = buildings_df[
                    (buildings_df['latitude'].isna()) | 
                    (buildings_df['longitude'].isna()) |
                    (buildings_df['latitude'] < 0.5) |
                    (buildings_df['latitude'] > 7.5) |
                    (buildings_df['longitude'] < 99.0) |
                    (buildings_df['longitude'] > 120.0)
                ]
                if len(invalid_coords) > 0:
                    anomalies.append(f"{len(invalid_coords)} b√¢timents avec coordonn√©es invalides")
            
            # Surfaces anormales
            if 'surface_area_m2' in buildings_df.columns:
                surfaces = buildings_df['surface_area_m2']
                
                # Surfaces nulles ou n√©gatives
                invalid_surfaces = (surfaces <= 0).sum()
                if invalid_surfaces > 0:
                    anomalies.append(f"{invalid_surfaces} b√¢timents avec surface invalide")
                
                # Surfaces extr√™mes
                very_large = (surfaces > 50000).sum()  # > 50,000 m¬≤
                if very_large > 0:
                    anomalies.append(f"{very_large} b√¢timents avec surface tr√®s importante (>50k m¬≤)")
            
            # Doublons d'IDs
            if 'unique_id' in buildings_df.columns:
                duplicates = buildings_df['unique_id'].duplicated().sum()
                if duplicates > 0:
                    anomalies.append(f"{duplicates} IDs de b√¢timents dupliqu√©s")
            
        except Exception as e:
            anomalies.append(f"Erreur d√©tection anomalies b√¢timents: {e}")
        
        return anomalies
    
    # =========================================================================
    # TEMPLATES D'ANALYSE
    # =========================================================================
    
    def _get_overview_template(self) -> str:
        return """Tu es un expert en analyse de donn√©es √©nerg√©tiques pour la Malaysia.

DONN√âES √Ä ANALYSER:
{stats_context}

CONTEXTE EXPERT:
{rag_context}

INSTRUCTIONS:
1. Analyse globale de la qualit√© et completude des donn√©es
2. Identification des tendances principales 
3. Points d'attention ou limitations
4. Premi√®re √©valuation de la performance √©nerg√©tique
5. Recommandations pour analyses plus pouss√©es

R√©ponds de mani√®re structur√©e et accessible, en te concentrant sur les insights les plus importants."""
    
    def _get_consumption_template(self) -> str:
        return """Tu es un analyste sp√©cialis√© en consommation √©nerg√©tique.

ANALYSE D√âTAILL√âE:
{analysis_context}

CONTEXTE EXPERT:
{rag_context}

INSTRUCTIONS:
1. Interpr√®te les patterns de consommation identifi√©s
2. √âvalue l'efficacit√© √©nerg√©tique par rapport aux standards
3. Identifie les opportunit√©s d'optimisation
4. Analyse les variations temporelles et leur signification
5. Compare avec les benchmarks industry si disponibles

Fournis une analyse approfondie avec des recommandations concr√®tes."""
    
    def _get_buildings_template(self) -> str:
        return """Tu es un expert en patrimoine immobilier et efficacit√© √©nerg√©tique.

ANALYSE PATRIMOINE:
{buildings_context}

CONTEXTE EXPERT:
{rag_context}

INSTRUCTIONS:
1. Analyse la composition du patrimoine b√¢timents
2. √âvalue la r√©partition g√©ographique et ses implications
3. Identifie les typologies dominantes et leurs caract√©ristiques
4. Analyse la distribution des surfaces et son impact √©nerg√©tique
5. Propose des strat√©gies d'optimisation par type/zone

Concentre-toi sur les implications √©nerg√©tiques et les opportunit√©s d'am√©lioration."""
    
    def _get_weather_template(self) -> str:
        return """Tu es un sp√©cialiste en corr√©lations m√©t√©o-√©nerg√©tiques.

DONN√âES M√âT√âOROLOGIQUES:
{weather_context}

CONTEXTE EXPERT:
{rag_context}

INSTRUCTIONS:
1. Analyse l'impact des conditions m√©t√©o sur la consommation
2. Identifie les corr√©lations significatives
3. √âvalue la sensibilit√© climatique du parc de b√¢timents
4. Propose des strat√©gies d'adaptation climatique
5. Recommande des mesures de r√©silience √©nerg√©tique

Focus sur les implications pratiques pour la gestion √©nerg√©tique."""
    
    def _get_trends_template(self) -> str:
        return """Tu es un analyste en tendances √©nerg√©tiques.

ANALYSE TEMPORELLE:
{trends_context}

CONTEXTE EXPERT:
{rag_context}

INSTRUCTIONS:
1. Identifie les tendances d'√©volution significatives
2. Analyse la saisonnalit√© et les cycles
3. D√©tecte les changements de comportement
4. Projette les √©volutions futures probables
5. Recommande des actions pr√©ventives ou correctives

Privil√©gie l'analyse pr√©dictive et l'aide √† la d√©cision."""
    
    def _get_anomalies_template(self) -> str:
        return """Tu es un expert en d√©tection d'anomalies √©nerg√©tiques.

ANOMALIES D√âTECT√âES:
{anomalies_context}

CONTEXTE EXPERT:
{rag_context}

INSTRUCTIONS:
1. Interpr√®te la signification des anomalies d√©tect√©es
2. √âvalue leur impact sur la fiabilit√© des analyses
3. Propose des actions correctives pour les donn√©es
4. Identifie les anomalies n√©cessitant une investigation
5. Recommande des processus de validation am√©lior√©s

Focus sur l'am√©lioration de la qualit√© des donn√©es."""
    
    def _get_recommendations_template(self) -> str:
        return """Tu es un consultant senior en optimisation √©nerg√©tique.

CONTEXTE GLOBAL:
{recommendations_context}

CONTEXTE EXPERT:
{rag_context}

INSTRUCTIONS:
1. Synth√©tise les opportunit√©s d'optimisation identifi√©es
2. Priorise les actions par impact/effort
3. Propose un plan d'action structur√©
4. Estime les b√©n√©fices potentiels
5. Identifie les risques et conditions de succ√®s

Fournis des recommandations strat√©giques actionnables avec ROI estim√©."""
    
    # =========================================================================
    # M√âTHODES UTILITAIRES SUPPL√âMENTAIRES
    # =========================================================================
    
    def _calculate_data_hash(self, data: Dict) -> str:
        """Calcule un hash unique des donn√©es pour √©viter les re-analyses"""
        import hashlib
        
        hash_components = []
        
        for key, df in data.items():
            if df is not None and hasattr(df, 'shape'):
                # Hash bas√© sur la forme et quelques valeurs
                hash_components.append(f"{key}:{df.shape}")
                if hasattr(df, 'columns'):
                    hash_components.append(str(list(df.columns)))
        
        combined = "|".join(hash_components)
        return hashlib.md5(combined.encode()).hexdigest()
    
    def _interpretation_exists(self, data_hash: str) -> bool:
        """V√©rifie si une interpr√©tation existe d√©j√† pour ce hash"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                    "SELECT id FROM interpretations WHERE data_hash = ? AND created_at > datetime('now', '-24 hours')",
                    (data_hash,)
                )
                return cursor.fetchone() is not None
        except:
            return False
    
    def _get_existing_interpretation(self, data_hash: str) -> Dict:
        """R√©cup√®re une interpr√©tation existante"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT interpretation_type, analysis_content, insights, 
                           recommendations, confidence_score, created_at
                    FROM interpretations 
                    WHERE data_hash = ? 
                    ORDER BY created_at DESC
                """, (data_hash,))
                
                rows = cursor.fetchall()
                
                interpretations = {}
                for row in rows:
                    interpretations[row[0]] = {
                        'type': row[0],
                        'content': row[1],
                        'insights': json.loads(row[2]) if row[2] else [],
                        'recommendations': json.loads(row[3]) if row[3] else [],
                        'confidence': row[4],
                        'timestamp': row[5]
                    }
                
                return {
                    'success': True,
                    'interpretations': interpretations,
                    'from_cache': True,
                    'cache_timestamp': rows[0][5] if rows else None
                }
                
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration cache: {e}")
            return {'success': False, 'error': str(e)}
    
    def _save_interpretation_results(self, data_hash: str, interpretations: Dict):
        """Sauvegarde les r√©sultats d'interpr√©tation"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                for interp_type, result in interpretations.items():
                    if isinstance(result, dict):
                        conn.execute("""
                            INSERT OR REPLACE INTO interpretations
                            (interpretation_type, data_hash, analysis_content, 
                             insights, recommendations, confidence_score, metadata)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, (
                            interp_type,
                            data_hash,
                            result.get('content', ''),
                            json.dumps(result.get('insights', []), ensure_ascii=False),
                            json.dumps(result.get('recommendations', []), ensure_ascii=False),
                            result.get('confidence', 0.5),
                            json.dumps(result.get('metadata', {}), ensure_ascii=False)
                        ))
                
                conn.commit()
                
        except Exception as e:
            logger.error(f"Erreur sauvegarde interpr√©tations: {e}")
    
    def _format_rag_context(self, rag_context: List[Dict]) -> str:
        """Formate le contexte RAG pour les prompts"""
        if not rag_context:
            return "Aucun contexte expert sp√©cifique trouv√©."
        
        formatted = "CONNAISSANCES EXPERTES PERTINENTES:\n"
        for i, item in enumerate(rag_context[:3], 1):
            content = item.get('content', '')[:300]  # Limiter √† 300 chars
            formatted += f"{i}. {content}...\n"
        
        return formatted
    
    def _extract_consumption_insights(self, consumption_df: pd.DataFrame) -> List[str]:
        """Extrait des insights automatiques de consommation"""
        insights = []
        
        try:
            if 'y' not in consumption_df.columns:
                return insights
            
            consumption = consumption_df['y']
            
            # Insight sur la variabilit√©
            cv = consumption.std() / consumption.mean() * 100
            if cv > 50:
                insights.append(f"Forte variabilit√© de consommation ({cv:.1f}% de coefficient de variation)")
            elif cv < 20:
                insights.append(f"Consommation stable ({cv:.1f}% de coefficient de variation)")
            
            # Insight sur la distribution
            skewness = consumption.skew()
            if abs(skewness) > 1:
                direction = "positive" if skewness > 0 else "n√©gative"
                insights.append(f"Distribution asym√©trique {direction} des consommations")
            
            # Insight sur les extremes
            q95 = consumption.quantile(0.95)
            q05 = consumption.quantile(0.05)
            ratio = q95 / q05 if q05 > 0 else float('inf')
            
            if ratio > 5:
                insights.append(f"Large √©cart entre consommations faibles et √©lev√©es (ratio 95/5: {ratio:.1f})")
            
        except Exception as e:
            logger.debug(f"Erreur extraction insights: {e}")
        
        return insights
    
    def _analyze_building_types(self, buildings_df: pd.DataFrame) -> str:
        """Analyse la r√©partition par type de b√¢timent"""
        try:
            if 'building_type' not in buildings_df.columns:
                return "Colonne building_type manquante"
            
            type_counts = buildings_df['building_type'].value_counts()
            total = len(buildings_df)
            
            analysis = "R√âPARTITION PAR TYPE:\n"
            for btype, count in type_counts.items():
                percentage = count / total * 100
                analysis += f"‚Ä¢ {btype}: {count} b√¢timents ({percentage:.1f}%)\n"
            
            # Type dominant
            dominant_type = type_counts.index[0]
            analysis += f"\nType dominant: {dominant_type} ({type_counts.iloc[0]/total*100:.1f}%)"
            
            return analysis
            
        except Exception as e:
            return f"Erreur analyse types: {e}"
    
    def _analyze_geographic_distribution(self, buildings_df: pd.DataFrame) -> str:
        """Analyse la distribution g√©ographique"""
        try:
            analysis = ""
            
            if 'zone_name' in buildings_df.columns:
                zone_counts = buildings_df['zone_name'].value_counts()
                analysis += "R√âPARTITION PAR ZONE:\n"
                for zone, count in zone_counts.head(5).items():
                    percentage = count / len(buildings_df) * 100
                    analysis += f"‚Ä¢ {zone}: {count} b√¢timents ({percentage:.1f}%)\n"
            
            if 'latitude' in buildings_df.columns and 'longitude' in buildings_df.columns:
                lat_range = buildings_df['latitude'].max() - buildings_df['latitude'].min()
                lng_range = buildings_df['longitude'].max() - buildings_df['longitude'].min()
                
                analysis += f"\n√âTENDUE G√âOGRAPHIQUE:\n"
                analysis += f"‚Ä¢ Latitude: {lat_range:.3f}¬∞ ({buildings_df['latitude'].min():.3f} √† {buildings_df['latitude'].max():.3f})\n"
                analysis += f"‚Ä¢ Longitude: {lng_range:.3f}¬∞ ({buildings_df['longitude'].min():.3f} √† {buildings_df['longitude'].max():.3f})"
            
            return analysis
            
        except Exception as e:
            return f"Erreur analyse g√©ographique: {e}"
    
    def _analyze_surface_distribution(self, buildings_df: pd.DataFrame) -> str:
        """Analyse la distribution des surfaces"""
        try:
            if 'surface_area_m2' not in buildings_df.columns:
                return "Colonne surface_area_m2 manquante"
            
            surfaces = buildings_df['surface_area_m2']
            
            analysis = f"""DISTRIBUTION DES SURFACES:
‚Ä¢ Superficie totale: {surfaces.sum():,.0f} m¬≤
‚Ä¢ Superficie moyenne: {surfaces.mean():.0f} m¬≤
‚Ä¢ Superficie m√©diane: {surfaces.median():.0f} m¬≤
‚Ä¢ Plus grand b√¢timent: {surfaces.max():,.0f} m¬≤
‚Ä¢ Plus petit b√¢timent: {surfaces.min():.0f} m¬≤

R√âPARTITION PAR TAILLE:"""
            
            # Cat√©gories de taille
            small = (surfaces < 200).sum()
            medium = ((surfaces >= 200) & (surfaces < 1000)).sum()
            large = ((surfaces >= 1000) & (surfaces < 5000)).sum()
            very_large = (surfaces >= 5000).sum()
            
            total = len(surfaces)
            analysis += f"""
‚Ä¢ Petits (<200 m¬≤): {small} ({small/total*100:.1f}%)
‚Ä¢ Moyens (200-1000 m¬≤): {medium} ({medium/total*100:.1f}%)
‚Ä¢ Grands (1000-5000 m¬≤): {large} ({large/total*100:.1f}%)
‚Ä¢ Tr√®s grands (‚â•5000 m¬≤): {very_large} ({very_large/total*100:.1f}%)"""
            
            return analysis
            
        except Exception as e:
            return f"Erreur analyse surfaces: {e}"
    
    def _detect_cross_dataset_anomalies(self, data: Dict) -> List[str]:
        """D√©tecte les incoh√©rences entre datasets"""
        anomalies = []
        
        try:
            buildings_df = data.get('buildings')
            consumption_df = data.get('consumption')
            
            if buildings_df is not None and consumption_df is not None:
                # IDs de b√¢timents incoh√©rents
                if 'unique_id' in buildings_df.columns and 'unique_id' in consumption_df.columns:
                    building_ids = set(buildings_df['unique_id'].unique())
                    consumption_ids = set(consumption_df['unique_id'].unique())
                    
                    missing_in_consumption = building_ids - consumption_ids
                    missing_in_buildings = consumption_ids - building_ids
                    
                    if missing_in_consumption:
                        anomalies.append(f"{len(missing_in_consumption)} b√¢timents sans donn√©es de consommation")
                    
                    if missing_in_buildings:
                        anomalies.append(f"{len(missing_in_buildings)} consommations sans b√¢timent correspondant")
            
        except Exception as e:
            anomalies.append(f"Erreur v√©rification coh√©rence: {e}")
        
        return anomalies
    
    def _parse_recommendations(self, llm_response: str) -> List[str]:
        """Parse les recommandations de la r√©ponse LLM"""
        recommendations = []
        
        try:
            lines = llm_response.split('\n')
            current_rec = ""
            
            for line in lines:
                line = line.strip()
                
                # D√©tection d'une nouvelle recommandation
                if (line.startswith('‚Ä¢') or line.startswith('-') or 
                    line.startswith('*') or any(word in line.lower() for word in 
                    ['recommande', 'sugg√®re', 'devrait', 'pourrait', 'optimiser'])):
                    
                    if current_rec:
                        recommendations.append(current_rec.strip())
                    
                    current_rec = line.lstrip('‚Ä¢-* ')
                elif current_rec and line:
                    current_rec += " " + line
            
            # Ajouter la derni√®re recommandation
            if current_rec:
                recommendations.append(current_rec.strip())
        
        except Exception as e:
            logger.debug(f"Erreur parsing recommandations: {e}")
        
        return recommendations[:10]  # Limiter √† 10 recommandations
    
    def _extract_key_metrics(self, interpretations: Dict) -> Dict:
        """Extrait les m√©triques cl√©s de toutes les analyses"""
        metrics = {}
        
        try:
            for analysis_type, analysis in interpretations.items():
                if isinstance(analysis, dict):
                    confidence = analysis.get('confidence', 0)
                    
                    # M√©triques par type d'analyse
                    if analysis_type == 'consumption':
                        stats = analysis.get('statistics', '')
                        # Extraire les valeurs num√©riques des stats
                        # (impl√©mentation simplifi√©e)
                        metrics['consumption_confidence'] = confidence
                    
                    elif analysis_type == 'buildings':
                        building_stats = analysis.get('building_statistics', {})
                        metrics['buildings_confidence'] = confidence
                    
                    elif analysis_type == 'anomalies':
                        anomalies_list = analysis.get('anomalies_list', [])
                        metrics['anomalies_count'] = len(anomalies_list)
                        metrics['data_quality_score'] = max(0, 100 - len(anomalies_list) * 5)
            
            # Score global de confiance
            confidences = [a.get('confidence', 0) for a in interpretations.values() 
                          if isinstance(a, dict)]
            if confidences:
                metrics['overall_confidence'] = sum(confidences) / len(confidences)
        
        except Exception as e:
            logger.debug(f"Erreur extraction m√©triques: {e}")
        
        return metrics
    
    def _calculate_overall_confidence(self, interpretations: Dict) -> float:
        """Calcule le score de confiance global"""
        try:
            confidences = []
            
            for analysis in interpretations.values():
                if isinstance(analysis, dict) and 'confidence' in analysis:
                    confidences.append(analysis['confidence'])
            
            if confidences:
                return sum(confidences) / len(confidences)
            else:
                return 0.5
                
        except:
            return 0.5
    
    def _generate_fallback_analysis(self, data_summary: Dict) -> str:
        """G√©n√®re une analyse de secours si le LLM √©choue"""
        fallback = f"""
ANALYSE AUTOMATIQUE DE SECOURS

R√©sum√© des donn√©es charg√©es:
‚Ä¢ {data_summary.get('total_buildings', 0)} b√¢timents
‚Ä¢ {data_summary.get('total_consumption', 0):,.0f} kWh de consommation totale  
‚Ä¢ Types: {', '.join(data_summary.get('building_types', []))}
‚Ä¢ Zones: {', '.join(data_summary.get('zones', []))}

Points d'attention:
‚Ä¢ V√©rifiez la qualit√© des donn√©es charg√©es
‚Ä¢ Assurez-vous que tous les fichiers n√©cessaires sont pr√©sents
‚Ä¢ Consultez les logs pour identifier les probl√®mes √©ventuels

Cette analyse simplifi√©e est g√©n√©r√©e car le service d'IA n'est pas disponible.
Relancez l'analyse une fois Ollama connect√©.
"""
        return fallback
    
    # =========================================================================
    # M√âTHODES PUBLIQUES ADDITIONNELLES
    # =========================================================================
    
    def get_interpretation_history(self, limit: int = 10) -> List[Dict]:
        """R√©cup√®re l'historique des interpr√©tations"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT interpretation_type, created_at, confidence_score,
                           substr(analysis_content, 1, 200) as preview
                    FROM interpretations
                    ORDER BY created_at DESC
                    LIMIT ?
                """, (limit,))
                
                history = []
                for row in cursor.fetchall():
                    history.append({
                        'type': row[0],
                        'timestamp': row[1],
                        'confidence': row[2],
                        'preview': row[3] + "..." if len(row[3]) == 200 else row[3]
                    })
                
                return history
                
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration historique: {e}")
            return []
    
    def get_interpretation_stats(self) -> Dict:
        """Statistiques des interpr√©tations"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # Nombre total
                cursor = conn.execute("SELECT COUNT(*) FROM interpretations")
                total = cursor.fetchone()[0]
                
                # Par type
                cursor = conn.execute("""
                    SELECT interpretation_type, COUNT(*), AVG(confidence_score)
                    FROM interpretations
                    GROUP BY interpretation_type
                """)
                by_type = cursor.fetchall()
                
                # Derni√®res 24h
                cursor = conn.execute("""
                    SELECT COUNT(*) FROM interpretations
                    WHERE created_at > datetime('now', '-24 hours')
                """)
                recent = cursor.fetchone()[0]
                
                return {
                    'total_interpretations': total,
                    'recent_24h': recent,
                    'by_type': {
                        row[0]: {
                            'count': row[1],
                            'avg_confidence': round(row[2], 2)
                        } for row in by_type
                    }
                }
                
        except Exception as e:
            logger.error(f"Erreur stats interpr√©tations: {e}")
            return {}
    
    def clear_old_interpretations(self, days_old: int = 30) -> int:
        """Supprime les anciennes interpr√©tations"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    DELETE FROM interpretations 
                    WHERE created_at < datetime('now', '-{} days')
                """.format(days_old))
                
                deleted = cursor.rowcount
                conn.commit()
                
                logger.info(f"üóëÔ∏è {deleted} anciennes interpr√©tations supprim√©es")
                return deleted
                
        except Exception as e:
            logger.error(f"Erreur nettoyage interpr√©tations: {e}")
            return 0


# ==============================================================================
# INT√âGRATION AVEC L'APPLICATION PRINCIPALE
# ==============================================================================

def create_interpreter_api_routes(app, data_interpreter):
    """Cr√©e les routes API pour l'interpr√©teur de donn√©es"""
    
    @app.route('/api/interpret/analyze', methods=['POST'])
    def interpret_data():
        """Lance l'interpr√©tation des donn√©es"""
        try:
            from flask import request, jsonify
            
            data = request.get_json() or {}
            force_refresh = data.get('force_refresh', False)
            
            # Lancement asynchrone (simulation)
            import asyncio
            result = asyncio.run(data_interpreter.interpret_loaded_data(force_refresh))
            
            return jsonify(result)
            
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/interpret/history')
    def interpretation_history():
        """Historique des interpr√©tations"""
        try:
            from flask import request, jsonify
            
            limit = int(request.args.get('limit', 10))
            history = data_interpreter.get_interpretation_history(limit)
            
            return jsonify({'success': True, 'history': history})
            
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/interpret/stats')
    def interpretation_stats():
        """Statistiques des interpr√©tations"""
        try:
            from flask import jsonify
            
            stats = data_interpreter.get_interpretation_stats()
            return jsonify({'success': True, 'stats': stats})
            
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500


# ==============================================================================
# TEST ET UTILISATION
# ==============================================================================

def test_data_interpreter():
    """Test de l'interpr√©teur de donn√©es"""
    
    # Simulation des services requis
    class MockOllamaService:
        def analyze_data(self, question, context, data_summary):
            return {
                'success': True,
                'analysis': {
                    'full_response': f"Analyse simul√©e pour: {question}",
                    'insights': ["Insight 1", "Insight 2"]
                }
            }
    
    class MockRAGService:
        def search_context(self, query, top_k=5):
            return [
                {'content': f"Contexte simul√© pour {query}", 'relevance_score': 0.8}
            ]
    
    class MockDataService:
        def get_current_data(self):
            return {
                'buildings': pd.DataFrame({
                    'unique_id': ['B1', 'B2'],
                    'building_type': ['residential', 'commercial'],
                    'surface_area_m2': [100, 500]
                }),
                'consumption': pd.DataFrame({
                    'unique_id': ['B1', 'B1', 'B2'],
                    'timestamp': pd.date_range('2024-01-01', periods=3, freq='H'),
                    'y': [10, 12, 25]
                })
            }
        
        def get_data_summary(self):
            return {
                'total_buildings': 2,
                'total_consumption': 47,
                'building_types': ['residential', 'commercial'],
                'period': '2024-01-01 √† 2024-01-02'
            }
    
    # Test
    interpreter = DataInterpreter(
        MockOllamaService(),
        MockRAGService(), 
        MockDataService()
    )
    
    print("üß™ Test DataInterpreter:")
    
    # Test interpr√©tation
    import asyncio
    result = asyncio.run(interpreter.interpret_loaded_data())
    
    print(f"‚úÖ Interpr√©tation: {result.get('success', False)}")
    if result.get('success'):
        print(f"üìä Analyses: {list(result.get('interpretations', {}).keys())}")
        print(f"üìã R√©sum√© ex√©cutif disponible: {'executive_summary' in result}")
        print(f"üéØ Confiance globale: {result.get('confidence_score', 0):.2f}")
    
    # Test historique
    history = interpreter.get_interpretation_history()
    print(f"üìö Historique: {len(history)} interpr√©tations")
    
    # Test statistiques
    stats = interpreter.get_interpretation_stats()
    print(f"üìà Stats: {stats.get('total_interpretations', 0)} interpr√©tations totales")
    
    return interpreter


if __name__ == '__main__':
    test_data_interpreter()